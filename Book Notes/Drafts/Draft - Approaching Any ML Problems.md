
---
annotation-target: https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf
---

>%%
>```annotation-json
>{"created":"2023-02-26T00:22:38.065Z","text":"Why we need to dual boot if we already have WSL?","updated":"2023-02-26T00:22:38.065Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":3790,"end":3824},{"type":"TextQuoteSelector","exact":"I prefer dual boot as it is native","prefix":" Windows as a dual boot system. ","suffix":". If you are not an Ubuntu user,"}]}]}
>```
>%%
>*%%PREFIX%%Windows as a dual boot system.%%HIGHLIGHT%% ==I prefer dual boot as it is native== %%POSTFIX%%. If you are not an Ubuntu user,*
>%%LINK%%[[#^co8mi8bdbeo|show annotation]]
>%%COMMENT%%
>Why we need to dual boot if we already have WSL?
>%%TAGS%%
>
^co8mi8bdbeo



>%%
>```annotation-json
>{"created":"2023-02-26T00:23:56.208Z","text":"Why Miniconda instead of Python and VirtualEnv Package?","updated":"2023-02-26T00:23:56.208Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":4092,"end":4101},{"type":"TextQuoteSelector","exact":"Miniconda","prefix":"h Anaconda. I particularly like ","suffix":", which is a minimal installer f"}]}]}
>```
>%%
>*%%PREFIX%%h Anaconda. I particularly like%%HIGHLIGHT%% ==Miniconda== %%POSTFIX%%, which is a minimal installer f*
>%%LINK%%[[#^keohc2ua9b|show annotation]]
>%%COMMENT%%
>Why Miniconda instead of Python and VirtualEnv Package?
>%%TAGS%%
>
^keohc2ua9b


>%%
>```annotation-json
>{"created":"2023-02-26T00:27:31.894Z","text":"can we just pip install this yml file in our venv?","updated":"2023-02-26T00:27:31.894Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":6232,"end":6268},{"type":"TextQuoteSelector","exact":"conda env create -f environment.yml ","prefix":"using the following command:  $ ","suffix":" This command will create an env"}]}]}
>```
>%%
>*%%PREFIX%%using the following command:  $%%HIGHLIGHT%% ==conda env create -f environment.yml== %%POSTFIX%%This command will create an env*
>%%LINK%%[[#^eflp34euxe|show annotation]]
>%%COMMENT%%
>can we just pip install this yml file in our venv?
>%%TAGS%%
>
^eflp34euxe


>%%
>```annotation-json
>{"created":"2023-02-26T00:31:11.071Z","text":"The difference between supervised and unsupervised data is about the existence of target variable.","updated":"2023-02-26T00:31:11.071Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":6831,"end":6958},{"type":"TextQuoteSelector","exact":"Supervised data: always has one or multiple targets associated with it. • Unsupervised data: does not have any target variable.","prefix":"and machine learning models): • ","suffix":"  A supervised problem is consid"}]}]}
>```
>%%
>*%%PREFIX%%and machine learning models): •%%HIGHLIGHT%% ==Supervised data: always has one or multiple targets associated with it. • Unsupervised data: does not have any target variable.== %%POSTFIX%%A supervised problem is consid*
>%%LINK%%[[#^ub88awhx4u8|show annotation]]
>%%COMMENT%%
>The difference between supervised and unsupervised data is about the existence of target variable.
>%%TAGS%%
>
^ub88awhx4u8


>%%
>```annotation-json
>{"created":"2023-02-26T00:32:05.612Z","text":"Why supervised problem is more easier? And is supervised problem is about **predicting **target variable?","updated":"2023-02-26T00:32:05.612Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":6960,"end":7127},{"type":"TextQuoteSelector","exact":"A supervised problem is considerably easier to tackle than an unsupervised one. A problem in which we are required to predict a value is known as a supervised problem.","prefix":" not have any target variable.  ","suffix":" For example, if the problem is "}]}]}
>```
>%%
>*%%PREFIX%%not have any target variable.%%HIGHLIGHT%% ==A supervised problem is considerably easier to tackle than an unsupervised one. A problem in which we are required to predict a value is known as a supervised problem.== %%POSTFIX%%For example, if the problem is*
>%%LINK%%[[#^548swkhawrj|show annotation]]
>%%COMMENT%%
>Why supervised problem is more easier? And is supervised problem is about **predicting **target variable?
>%%TAGS%%
>
^548swkhawrj


>%%
>```annotation-json
>{"created":"2023-02-26T00:34:33.182Z","text":"columns -> features\nrows -> samples","updated":"2023-02-26T00:34:33.182Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":7739,"end":7848},{"type":"TextQuoteSelector","exact":"The columns are different features and rows represent different data points which are usually called samples.","prefix":"ociated with a target or label. ","suffix":" The example shows ten samples w"}]}]}
>```
>%%
>*%%PREFIX%%ociated with a target or label.%%HIGHLIGHT%% ==The columns are different features and rows represent different data points which are usually called samples.== %%POSTFIX%%The example shows ten samples w*
>%%LINK%%[[#^h45oogw1qx9|show annotation]]
>%%COMMENT%%
>columns -> features
>rows -> samples
>%%TAGS%%
>
^h45oogw1qx9


>%%
>```annotation-json
>{"created":"2023-02-26T00:35:50.443Z","text":"maybe he's talking about logistic regression? but what is it going to do with evaluation metric?","updated":"2023-02-26T00:35:50.443Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":8274,"end":8511},{"type":"TextQuoteSelector","exact":"Classification: predicting a category, e.g. dog or cat. • Regression: predicting a value, e.g. house prices.  It must be noted that sometimes we might use regression in a classification setting depending on the metric used for evaluation","prefix":"ivided into two sub-classes:  • ","suffix":". But we will come to that later"}]}]}
>```
>%%
>*%%PREFIX%%ivided into two sub-classes:  •%%HIGHLIGHT%% ==Classification: predicting a category, e.g. dog or cat. • Regression: predicting a value, e.g. house prices.  It must be noted that sometimes we might use regression in a classification setting depending on the metric used for evaluation== %%POSTFIX%%. But we will come to that later*
>%%LINK%%[[#^zk8vx9s72a|show annotation]]
>%%COMMENT%%
>maybe he's talking about logistic regression? but what is it going to do with evaluation metric?
>%%TAGS%%
>
^zk8vx9s72a


>%%
>```annotation-json
>{"created":"2023-02-26T00:37:53.923Z","text":"Clustering is the most popular approaches in unsupervised problems?","updated":"2023-02-26T00:37:53.923Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":9288,"end":9481},{"type":"TextQuoteSelector","exact":"Clustering is one of the approaches that you can use for problems like this, but it must be noted that there are several other approaches available that can be applied to unsupervised problems.","prefix":"sters can data be divided into. ","suffix":" For a fraud detection problem, "}]}]}
>```
>%%
>*%%PREFIX%%sters can data be divided into.%%HIGHLIGHT%% ==Clustering is one of the approaches that you can use for problems like this, but it must be noted that there are several other approaches available that can be applied to unsupervised problems.== %%POSTFIX%%For a fraud detection problem,*
>%%LINK%%[[#^mlvwua5f82d|show annotation]]
>%%COMMENT%%
>Clustering is the most popular approaches in unsupervised problems?
>%%TAGS%%
>
^mlvwua5f82d


>%%
>```annotation-json
>{"created":"2023-02-26T00:38:57.364Z","text":"what are decomposition techniques?","updated":"2023-02-26T00:38:57.364Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":10004,"end":10213},{"type":"TextQuoteSelector","exact":"To  make  sense  of  unsupervised  problems,  we  can  also  use numerous  decomposition  techniques  such  as  Principal  Component  Analysis (PCA), t-distributed Stochastic Neighbour Embedding (t-SNE) etc.  ","prefix":"ween the two assumed  targets.  ","suffix":" Supervised problems are easier "}]}]}
>```
>%%
>*%%PREFIX%%ween the two assumed  targets.%%HIGHLIGHT%% ==To  make  sense  of  unsupervised  problems,  we  can  also  use numerous  decomposition  techniques  such  as  Principal  Component  Analysis (PCA), t-distributed Stochastic Neighbour Embedding (t-SNE) etc.== %%POSTFIX%%Supervised problems are easier*
>%%LINK%%[[#^699a83p76xb|show annotation]]
>%%COMMENT%%
>what are decomposition techniques?
>%%TAGS%%
>
^699a83p76xb



>%%
>```annotation-json
>{"created":"2023-02-26T00:39:52.537Z","text":"Supervised problems are more easier because it has evaluation metrics.","updated":"2023-02-26T00:39:52.537Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":10214,"end":10302},{"type":"TextQuoteSelector","exact":"Supervised problems are easier to tackle in the sense that they can be evaluated easily.","prefix":"ghbour Embedding (t-SNE) etc.   ","suffix":" We will read more about evaluat"}]}]}
>```
>%%
>*%%PREFIX%%ghbour Embedding (t-SNE) etc.%%HIGHLIGHT%% ==Supervised problems are easier to tackle in the sense that they can be evaluated easily.== %%POSTFIX%%We will read more about evaluat*
>%%LINK%%[[#^fvrzyak01md|show annotation]]
>%%COMMENT%%
>Supervised problems are more easier because it has evaluation metrics.
>%%TAGS%%
>
^fvrzyak01md


>%%
>```annotation-json
>{"created":"2023-02-26T00:43:11.719Z","text":"You can convert a supervised dataset to unsupervised dataset.","updated":"2023-02-26T00:43:11.719Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":11678,"end":11801},{"type":"TextQuoteSelector","exact":"Most of the time, it’s also possible to convert a supervised dataset to unsupervised to see how they look like when plotted","prefix":"ust clustering several images.  ","suffix":".  For example, let’s take a loo"}]}]}
>```
>%%
>*%%PREFIX%%ust clustering several images.%%HIGHLIGHT%% ==Most of the time, it’s also possible to convert a supervised dataset to unsupervised to see how they look like when plotted== %%POSTFIX%%.  For example, let’s take a loo*
>%%LINK%%[[#^ey3unverqzp|show annotation]]
>%%COMMENT%%
>You can convert a supervised dataset to unsupervised dataset.
>%%TAGS%%
>
^ey3unverqzp


>%%
>```annotation-json
>{"created":"2023-02-26T00:46:11.657Z","text":"We use two components in t-SNE because of we want to visualize the data in two dimensional settings.","updated":"2023-02-26T00:46:11.657Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":15236,"end":15378},{"type":"TextQuoteSelector","exact":"This step creates the t-SNE transformation of the data. We use only two components as we can visualize them well in a two-dimensional setting.","prefix":"══════════════════════════════  ","suffix":" The transformed_data, in this c"}]}]}
>```
>%%
>*%%PREFIX%%══════════════════════════════%%HIGHLIGHT%% ==This step creates the t-SNE transformation of the data. We use only two components as we can visualize them well in a two-dimensional setting.== %%POSTFIX%%The transformed_data, in this c*
>%%LINK%%[[#^6fs2ct8iof9|show annotation]]
>%%COMMENT%%
>We use two components in t-SNE because of we want to visualize the data in two dimensional settings.
>%%TAGS%%
>
^6fs2ct8iof9


>%%
>```annotation-json
>{"created":"2023-02-26T00:47:30.531Z","text":"what is `np.column_stack()`","updated":"2023-02-26T00:47:30.531Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":15639,"end":15756},{"type":"TextQuoteSelector","exact":"tsne_df = pd.DataFrame(     np.column_stack((transformed_data, targets[:3000])),      columns=[\"x\", \"y\", \"targets\"] )","prefix":"═══════════════════════════════ ","suffix":"  tsne_df.loc[:, \"targets\"] = ts"}]}]}
>```
>%%
>*%%PREFIX%%═══════════════════════════════%%HIGHLIGHT%% ==tsne_df = pd.DataFrame(     np.column_stack((transformed_data, targets[:3000])),      columns=["x", "y", "targets"] )== %%POSTFIX%%tsne_df.loc[:, "targets"] = ts*
>%%LINK%%[[#^p5vce5mob1p|show annotation]]
>%%COMMENT%%
>what is `np.column_stack()`
>%%TAGS%%
>
^p5vce5mob1p


>%%
>```annotation-json
>{"created":"2023-02-26T00:49:05.936Z","text":"to find the optimal k in k-means clustering only by cross-validation? what about elbow method, sillohoute scores? are they includ in cross-validation?","updated":"2023-02-26T00:49:05.936Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":16771,"end":16958},{"type":"TextQuoteSelector","exact":"One question that arises all the time is how to find the optimal number of clusters in k-means clustering. Well, there is no right answer. You have to find the number by cross-validation.","prefix":"rms in an unsupervised setting. ","suffix":" Cross-validation will be discus"}]}]}
>```
>%%
>*%%PREFIX%%rms in an unsupervised setting.%%HIGHLIGHT%% ==One question that arises all the time is how to find the optimal number of clusters in k-means clustering. Well, there is no right answer. You have to find the number by cross-validation.== %%POSTFIX%%Cross-validation will be discus*
>%%LINK%%[[#^hpwh4j8b6p|show annotation]]
>%%COMMENT%%
>to find the optimal k in k-means clustering only by cross-validation? what about elbow method, sillohoute scores? are they includ in cross-validation?
>%%TAGS%%
>
^hpwh4j8b6p


>%%
>```annotation-json
>{"created":"2023-02-26T00:51:30.472Z","text":"if using supervised methods is better, why we need it to convert it using unsupervised methods?","updated":"2023-02-26T00:51:30.472Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":17556,"end":17626},{"type":"TextQuoteSelector","exact":"The results would be even better if we use classification algorithms. ","prefix":" with decomposition with t-SNE. ","suffix":"What are they and how to use the"}]}]}
>```
>%%
>*%%PREFIX%%with decomposition with t-SNE.%%HIGHLIGHT%% ==The results would be even better if we use classification algorithms.== %%POSTFIX%%What are they and how to use the*
>%%LINK%%[[#^ai73msxga9|show annotation]]
>%%COMMENT%%
>if using supervised methods is better, why we need it to convert it using unsupervised methods?
>%%TAGS%%
>
^ai73msxga9


>%%
>```annotation-json
>{"created":"2023-02-26T00:53:06.357Z","text":"so we do cross-validation before training the model or after it?","updated":"2023-02-26T00:53:06.357Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":18202,"end":18383},{"type":"TextQuoteSelector","exact":"cross-validation is a step in the process of building a machine learning model which helps us ensure that our models fit the data accurately and also ensures that we do not overfit.","prefix":"dation is. Mine is a one-liner: ","suffix":" But this leads to another term:"}]}]}
>```
>%%
>*%%PREFIX%%dation is. Mine is a one-liner:%%HIGHLIGHT%% ==cross-validation is a step in the process of building a machine learning model which helps us ensure that our models fit the data accurately and also ensures that we do not overfit.== %%POSTFIX%%But this leads to another term:*
>%%LINK%%[[#^a8gp2fpc1qw|show annotation]]
>%%COMMENT%%
>so we do cross-validation before training the model or after it?
>%%TAGS%%
>
^a8gp2fpc1qw


>%%
>```annotation-json
>{"created":"2023-02-26T00:54:48.101Z","text":"If the target variable is ordinal number, which is a categorical data, how we use regression method to solve it?","updated":"2023-02-26T00:54:48.101Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":19560,"end":19713},{"type":"TextQuoteSelector","exact":"We can treat this problem either as a classification problem or as a regression problem since wine quality is nothing but a real number between 0 and 10.","prefix":" the red wine quality dataset.  ","suffix":" For simplicity, let’s choose cl"}]}]}
>```
>%%
>*%%PREFIX%%the red wine quality dataset.%%HIGHLIGHT%% ==We can treat this problem either as a classification problem or as a regression problem since wine quality is nothing but a real number between 0 and 10.== %%POSTFIX%%For simplicity, let’s choose cl*
>%%LINK%%[[#^p85r13ef79|show annotation]]
>%%COMMENT%%
>If the target variable is ordinal number, which is a categorical data, how we use regression method to solve it?
>%%TAGS%%
>
^p85r13ef79


>%%
>```annotation-json
>{"created":"2023-02-26T00:57:28.460Z","text":"why not jump right into neural network approach? ","updated":"2023-02-26T00:57:28.460Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":20511,"end":20764},{"type":"TextQuoteSelector","exact":" a  lot  of algorithms come to our mind that we can apply to it, probably, we can use neural networks. But it would be a bit of a stretch if we dive into neural networks from the beginning. So, let’s start with something simple that we can visualize too","prefix":"it  a  classification  problem, ","suffix":": decision trees.  Before we beg"}]}]}
>```
>%%
>*%%PREFIX%%it  a  classification  problem,%%HIGHLIGHT%% ==a  lot  of algorithms come to our mind that we can apply to it, probably, we can use neural networks. But it would be a bit of a stretch if we dive into neural networks from the beginning. So, let’s start with something simple that we can visualize too== %%POSTFIX%%: decision trees.  Before we beg*
>%%LINK%%[[#^v0mouiqn44m|show annotation]]
>%%COMMENT%%
>why not jump right into neural network approach? 
>%%TAGS%%
>
^v0mouiqn44m


>%%
>```annotation-json
>{"created":"2023-02-26T00:58:24.935Z","text":"before split the data, shuffle it first","updated":"2023-02-26T00:58:24.935Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":21103,"end":21151},{"type":"TextQuoteSelector","exact":"use sample with frac=1 to shuffle the dataframe ","prefix":"═════════════════════════════ # ","suffix":"# we reset the indices since the"}]}]}
>```
>%%
>*%%PREFIX%%═════════════════════════════ #%%HIGHLIGHT%% ==use sample with frac=1 to shuffle the dataframe== %%POSTFIX%%# we reset the indices since the*
>%%LINK%%[[#^85u27inj15m|show annotation]]
>%%COMMENT%%
>before split the data, shuffle it first
>%%TAGS%%
>
^85u27inj15m


>%%
>```annotation-json
>{"created":"2023-02-26T01:00:43.270Z","text":"calculate accuracy for prediction in training set and test set.","updated":"2023-02-26T01:00:43.270Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":22642,"end":22718},{"type":"TextQuoteSelector","exact":"Now, we test the accuracy of this model on the training set and the test set","prefix":"is model to its default value.  ","suffix":":  ═════════════════════════════"}]}]}
>```
>%%
>*%%PREFIX%%is model to its default value.%%HIGHLIGHT%% ==Now, we test the accuracy of this model on the training set and the test set== %%POSTFIX%%:  ═════════════════════════════*
>%%LINK%%[[#^epml4qo92o7|show annotation]]
>%%COMMENT%%
>calculate accuracy for prediction in training set and test set.
>%%TAGS%%
>
^epml4qo92o7


>%%
>```annotation-json
>{"created":"2023-02-26T01:01:30.366Z","text":"How to chose the right metric?","updated":"2023-02-26T01:01:30.366Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":23526,"end":23658},{"type":"TextQuoteSelector","exact":"Here, we have used accuracy, mainly because it is the most straightforward metric. It might not be the best metric for this problem.","prefix":".6% and test accuracy of 57.3%. ","suffix":" What about we calculate these a"}]}]}
>```
>%%
>*%%PREFIX%%.6% and test accuracy of 57.3%.%%HIGHLIGHT%% ==Here, we have used accuracy, mainly because it is the most straightforward metric. It might not be the best metric for this problem.== %%POSTFIX%%What about we calculate these a*
>%%LINK%%[[#^enq4px5ye1|show annotation]]
>%%COMMENT%%
>How to chose the right metric?
>%%TAGS%%
>
^enq4px5ye1


>%%
>```annotation-json
>{"created":"2023-02-26T01:04:43.185Z","text":"definition of overfitting.","updated":"2023-02-26T01:04:43.185Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":26702,"end":26899},{"type":"TextQuoteSelector","exact":"The model fits perfectly on the training set and performs poorly when it comes to the test set. This means that the model will learn the training data well but will not generalize on unseen samples","prefix":"0 This is called overfitting.   ","suffix":". In the dataset above, one can "}]}]}
>```
>%%
>*%%PREFIX%%0 This is called overfitting.%%HIGHLIGHT%% ==The model fits perfectly on the training set and performs poorly when it comes to the test set. This means that the model will learn the training data well but will not generalize on unseen samples== %%POSTFIX%%. In the dataset above, one can*
>%%LINK%%[[#^48y3vl5vclm|show annotation]]
>%%COMMENT%%
>definition of overfitting.
>%%TAGS%%
>
^48y3vl5vclm


>%%
>```annotation-json
>{"created":"2023-02-26T01:05:53.649Z","text":"overfitting in neural network","updated":"2023-02-26T01:05:53.649Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":27333,"end":27440},{"type":"TextQuoteSelector","exact":"Another definition of overfitting would be when the test loss increases as we keep improving training loss.","prefix":" more or less remains the same. ","suffix":" This is very common when it com"}]}]}
>```
>%%
>*%%PREFIX%%more or less remains the same.%%HIGHLIGHT%% ==Another definition of overfitting would be when the test loss increases as we keep improving training loss.== %%POSTFIX%%This is very common when it com*
>%%LINK%%[[#^fcqg62q0au|show annotation]]
>%%COMMENT%%
>overfitting in neural network
>%%TAGS%%
>
^fcqg62q0au


>%%
>```annotation-json
>{"created":"2023-02-26T01:09:03.949Z","text":"when to stop training in neural network","updated":"2023-02-26T01:09:03.949Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":27952,"end":28027},{"type":"TextQuoteSelector","exact":" We must stop training where the validation loss reaches its minimum value.","prefix":"training loss decreases further.","suffix":"   This is the most common expla"}]}]}
>```
>%%
>*%%PREFIX%%training loss decreases further.%%HIGHLIGHT%% ==We must stop training where the validation loss reaches its minimum value.== %%POSTFIX%%This is the most common expla*
>%%LINK%%[[#^5jiaoq92nog|show annotation]]
>%%COMMENT%%
>when to stop training in neural network
>%%TAGS%%
>
^5jiaoq92nog


>%%
>```annotation-json
>{"created":"2023-02-26T01:12:55.558Z","text":"What is Occam's Razor?","updated":"2023-02-26T01:12:55.558Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":28358,"end":28446},{"type":"TextQuoteSelector","exact":"In general, whenever your model does not obey Occam’s razor, it is probably overfitting.","prefix":"e most generalizable solutions. ","suffix":"   Figure 3: Most general defini"}]}]}
>```
>%%
>*%%PREFIX%%e most generalizable solutions.%%HIGHLIGHT%% ==In general, whenever your model does not obey Occam’s razor, it is probably overfitting.== %%POSTFIX%%Figure 3: Most general defini*
>%%LINK%%[[#^pmbn0pwcq2|show annotation]]
>%%COMMENT%%
>What is Occam's Razor?
>%%TAGS%%
>
^pmbn0pwcq2


>%%
>```annotation-json
>{"created":"2023-02-26T03:04:46.696Z","text":"spitting datasets into train and test sets is also a kind of  cross-validation.","updated":"2023-02-26T03:04:46.696Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":28701,"end":28905},{"type":"TextQuoteSelector","exact":"Well, this is also a kind of cross-validation commonly known as a hold-out set. We use this kind of (cross-) validation when we have a large amount of data and model inference is a time-consuming process.","prefix":" performance on the other part. ","suffix":"  There are many different ways "}]}]}
>```
>%%
>*%%PREFIX%%performance on the other part.%%HIGHLIGHT%% ==Well, this is also a kind of cross-validation commonly known as a hold-out set. We use this kind of (cross-) validation when we have a large amount of data and model inference is a time-consuming process.== %%POSTFIX%%There are many different ways*
>%%LINK%%[[#^nk3s24ohb8|show annotation]]
>%%COMMENT%%
>spitting datasets into train and test sets is also a kind of  cross-validation.
>%%TAGS%%
>
^nk3s24ohb8


>%%
>```annotation-json
>{"created":"2023-02-26T03:06:00.099Z","text":"the aim of cross-validation.","updated":"2023-02-26T03:06:00.099Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":29076,"end":29089},{"type":"TextQuoteSelector","exact":"generalizable","prefix":"ine  learning  model  which  is ","suffix":" when it comes to unseen data. C"}]}]}
>```
>%%
>*%%PREFIX%%ine  learning  model  which  is%%HIGHLIGHT%% ==generalizable== %%POSTFIX%%when it comes to unseen data. C*
>%%LINK%%[[#^frughbf9cjt|show annotation]]
>%%COMMENT%%
>the aim of cross-validation.
>%%TAGS%%
>
^frughbf9cjt



>%%
>```annotation-json
>{"created":"2023-02-26T03:15:16.979Z","text":"how cross validation works","updated":"2023-02-26T03:15:16.979Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":29646,"end":29780},{"type":"TextQuoteSelector","exact":"Cross-validation is dividing training data into a few parts. We train the model on some of these parts and test on the remaining parts","prefix":" group k-fold cross-validation  ","suffix":". Take a look at figure 4.   Fig"}]}]}
>```
>%%
>*%%PREFIX%%group k-fold cross-validation%%HIGHLIGHT%% ==Cross-validation is dividing training data into a few parts. We train the model on some of these parts and test on the remaining parts== %%POSTFIX%%. Take a look at figure 4.   Fig*
>%%LINK%%[[#^4i1gtp80518|show annotation]]
>%%COMMENT%%
>how cross validation works
>%%TAGS%%
>
^4i1gtp80518


>%%
>```annotation-json
>{"created":"2023-02-26T03:16:08.796Z","text":"the most popular cross-validation techniques","updated":"2023-02-26T03:16:08.796Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":29419,"end":29480},{"type":"TextQuoteSelector","exact":"k-fold cross-validation • stratified k-fold cross-validation ","prefix":"idely used.   These include:  • ","suffix":"Approaching (Almost) Any Machine"}]}]}
>```
>%%
>*%%PREFIX%%idely used.   These include:  •%%HIGHLIGHT%% ==k-fold cross-validation • stratified k-fold cross-validation== %%POSTFIX%%Approaching (Almost) Any Machine*
>%%LINK%%[[#^8y8gjp60vea|show annotation]]
>%%COMMENT%%
>the most popular cross-validation techniques
>%%TAGS%%
>
^8y8gjp60vea


>%%
>```annotation-json
>{"created":"2023-02-26T03:17:16.012Z","text":"why split the validation set to test set again?","updated":"2023-02-26T03:17:16.012Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":30021,"end":30087},{"type":"TextQuoteSelector","exact":"Many people also split it into a third set and call it a test set.","prefix":" sets: training and validation. ","suffix":" We will, however, be using only"}]}]}
>```
>%%
>*%%PREFIX%%sets: training and validation.%%HIGHLIGHT%% ==Many people also split it into a third set and call it a test set.== %%POSTFIX%%We will, however, be using only*
>%%LINK%%[[#^c11lfsgh4pw|show annotation]]
>%%COMMENT%%
>why split the validation set to test set again?
>%%TAGS%%
>
^c11lfsgh4pw


>%%
>```annotation-json
>{"created":"2023-02-26T03:17:54.299Z","text":"The right cross-validation technique is depends on your dataset.","updated":"2023-02-26T03:17:54.299Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":29120,"end":29200},{"type":"TextQuoteSelector","exact":"Choosing the right cross-validation depends on the dataset you are dealing with,","prefix":"e when it comes to unseen data. ","suffix":" and one’s choice of cross-valid"}]}]}
>```
>%%
>*%%PREFIX%%e when it comes to unseen data.%%HIGHLIGHT%% ==Choosing the right cross-validation depends on the dataset you are dealing with,== %%POSTFIX%%and one’s choice of cross-valid*
>%%LINK%%[[#^rysiin0rcqq|show annotation]]
>%%COMMENT%%
>The right cross-validation technique is depends on your dataset.
>%%TAGS%%
>
^rysiin0rcqq


>%%
>```annotation-json
>{"created":"2023-02-26T03:29:35.185Z","text":"k-folds validation technique can use to almost all kinds of datasets.","updated":"2023-02-26T03:29:35.185Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":31490,"end":31549},{"type":"TextQuoteSelector","exact":"You can use this process with almost all kinds of datasets.","prefix":"══════════════════════════════  ","suffix":" For example, when you have imag"}]}]}
>```
>%%
>*%%PREFIX%%══════════════════════════════%%HIGHLIGHT%% ==You can use this process with almost all kinds of datasets.== %%POSTFIX%%For example, when you have imag*
>%%LINK%%[[#^igqf93vc0v8|show annotation]]
>%%COMMENT%%
>k-folds validation technique can use to almost all kinds of datasets.
>%%TAGS%%
>
^igqf93vc0v8


>%%
>```annotation-json
>{"created":"2023-02-26T03:31:18.879Z","text":"Use Stratified K-Fold validation technique when your data is skewed or imbalance.","updated":"2023-02-26T03:31:18.879Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":32089,"end":32253},{"type":"TextQuoteSelector","exact":"Stratified k-fold cross-validation keeps the ratio of labels in each fold constant. So, in each fold, you will have the same 90% positive and 10% negative samples. ","prefix":"tified k-fold cross-validation. ","suffix":"Thus, whatever metric you choose"}]}]}
>```
>%%
>*%%PREFIX%%tified k-fold cross-validation.%%HIGHLIGHT%% ==Stratified k-fold cross-validation keeps the ratio of labels in each fold constant. So, in each fold, you will have the same 90% positive and 10% negative samples.== %%POSTFIX%%Thus, whatever metric you choose*
>%%LINK%%[[#^wef38z3jcyl|show annotation]]
>%%COMMENT%%
>Use Stratified K-Fold validation technique when your data is skewed or imbalance.
>%%TAGS%%
>
^wef38z3jcyl


>%%
>```annotation-json
>{"created":"2023-02-26T03:36:55.399Z","text":"The default is to use stratified k-fold all the time when facing standard classification problem.","updated":"2023-02-26T03:36:55.399Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":34541,"end":34637},{"type":"TextQuoteSelector","exact":"The rule is simple. If it’s a standard classification problem, choose stratified k-fold blindly.","prefix":"n of “quality” in wine dataset  ","suffix":"  But what should we do if we ha"}]}]}
>```
>%%
>*%%PREFIX%%n of “quality” in wine dataset%%HIGHLIGHT%% ==The rule is simple. If it’s a standard classification problem, choose stratified k-fold blindly.== %%POSTFIX%%But what should we do if we ha*
>%%LINK%%[[#^igifyhh8im9|show annotation]]
>%%COMMENT%%
>The default is to use stratified k-fold all the time when facing standard classification problem.
>%%TAGS%%
>
^igifyhh8im9


>%%
>```annotation-json
>{"created":"2023-02-26T03:46:00.299Z","text":"hold-out based validation technique is suitable for big datasets (ex: 1 million samples).","updated":"2023-02-26T03:46:00.299Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":34818,"end":35005},{"type":"TextQuoteSelector","exact":"epending on which algorithm we choose, training and even validation can be very expensive for a dataset which is of this size. In these cases, we can opt for a hold-out based validation. ","prefix":"amples and validating on 200k. D","suffix":" The process for creating the ho"}]}]}
>```
>%%
>*%%PREFIX%%amples and validating on 200k. D%%HIGHLIGHT%% ==epending on which algorithm we choose, training and even validation can be very expensive for a dataset which is of this size. In these cases, we can opt for a hold-out based validation.== %%POSTFIX%%The process for creating the ho*
>%%LINK%%[[#^f2n7qyyubbq|show annotation]]
>%%COMMENT%%
>hold-out based validation technique is suitable for big datasets (ex: 1 million samples).
>%%TAGS%%
>
^f2n7qyyubbq


>%%
>```annotation-json
>{"created":"2023-02-26T03:50:35.694Z","text":"Hold-out based validation technique is commonly used in time-series data.","updated":"2023-02-26T03:50:35.694Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":35360,"end":35421},{"type":"TextQuoteSelector","exact":"Hold-out is also used very frequently with time-series  data.","prefix":"set and train on 900k samples.  ","suffix":" Let’s assume the problem we are"}]}]}
>```
>%%
>*%%PREFIX%%set and train on 900k samples.%%HIGHLIGHT%% ==Hold-out is also used very frequently with time-series  data.== %%POSTFIX%%Let’s assume the problem we are*
>%%LINK%%[[#^sguj37xy92o|show annotation]]
>%%COMMENT%%
>Hold-out based validation technique is commonly used in time-series data.
>%%TAGS%%
>
^sguj37xy92o


>%%
>```annotation-json
>{"created":"2023-02-26T03:53:10.804Z","text":"when predicting new datasets, you should include the hold-out based data to your model. Otherwise, performance will be sub-par.","updated":"2023-02-26T03:53:10.804Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":35981,"end":36138},{"type":"TextQuoteSelector","exact":" You should note that when you are predicting from 31 to 40, you should include the data from 21 to 30 in your model; otherwise, performance will be sub-par.","prefix":"ur model from step 0 to step 20.","suffix":"  In many cases, we have to deal"}]}]}
>```
>%%
>*%%PREFIX%%ur model from step 0 to step 20.%%HIGHLIGHT%% ==You should note that when you are predicting from 31 to 40, you should include the data from 21 to 30 in your model; otherwise, performance will be sub-par.== %%POSTFIX%%In many cases, we have to deal*
>%%LINK%%[[#^ufja9fwnl4q|show annotation]]
>%%COMMENT%%
>when predicting new datasets, you should include the hold-out based data to your model. Otherwise, performance will be sub-par.
>%%TAGS%%
>
^ufja9fwnl4q


>%%
>```annotation-json
>{"created":"2023-02-26T04:02:44.949Z","text":"Choosing k in k-fold cross validation when the dataset is small.","updated":"2023-02-26T04:02:44.949Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":36140,"end":36490},{"type":"TextQuoteSelector","exact":"In many cases, we have to deal with small datasets and creating big validation sets means losing a lot of data for the model to learn. In those cases, we can opt for a type of k-fold cross-validation where k=N, where N is the number of samples in the dataset. This means that in all folds of training, we will be training on all data samples except 1","prefix":", performance will be sub-par.  ","suffix":". The number of folds for this t"}]}]}
>```
>%%
>*%%PREFIX%%, performance will be sub-par.%%HIGHLIGHT%% ==In many cases, we have to deal with small datasets and creating big validation sets means losing a lot of data for the model to learn. In those cases, we can opt for a type of k-fold cross-validation where k=N, where N is the number of samples in the dataset. This means that in all folds of training, we will be training on all data samples except 1== %%POSTFIX%%. The number of folds for this t*
>%%LINK%%[[#^6jelkzu44jh|show annotation]]
>%%COMMENT%%
>Choosing k in k-fold cross validation when the dataset is small.
>%%TAGS%%
>
^6jelkzu44jh


>%%
>```annotation-json
>{"created":"2023-02-26T04:03:51.668Z","text":"the k=N approach will takes time but for small datasets that't the only option.","updated":"2023-02-26T04:03:51.668Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":36635,"end":36846},{"type":"TextQuoteSelector","exact":"this type of cross-validation can be costly in terms of the time it takes if the model is not fast enough, but since it’s only preferable to use this cross-validation for small datasets, it doesn’t matter much. ","prefix":"dataset.   One should note that ","suffix":" Now we can move to regression. "}]}]}
>```
>%%
>*%%PREFIX%%dataset.   One should note that%%HIGHLIGHT%% ==this type of cross-validation can be costly in terms of the time it takes if the model is not fast enough, but since it’s only preferable to use this cross-validation for small datasets, it doesn’t matter much.== %%POSTFIX%%Now we can move to regression.*
>%%LINK%%[[#^z7qi9f10qbh|show annotation]]
>%%COMMENT%%
>the k=N approach will takes time but for small datasets that't the only option.
>%%TAGS%%
>
^z7qi9f10qbh


>%%
>```annotation-json
>{"created":"2023-02-26T04:05:44.893Z","text":"in regression, we can use all the cross-validation technique except for stratified k-fold (we cannot use it directly, mostly we use simple k-fold cross validation for regression, but if the distribution of targets is not consistent, we can use stratified k-fold).","updated":"2023-02-26T04:05:44.893Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":36878,"end":37378},{"type":"TextQuoteSelector","exact":"The good thing about regression problems is that we can use all the cross-validation techniques mentioned above for regression problems except for stratified k-fold. That is we cannot use stratified k-fold directly, but there are ways to change the problem a bit so that we can use stratified k-fold for regression problems. Mostly, simple k-fold cross-validation works for any regression problem. However, if you see that the distribution of targets is not consistent, you can use stratified k-fold.","prefix":" Now we can move to regression. ","suffix":"  Approaching (Almost) Any Machi"}]}]}
>```
>%%
>*%%PREFIX%%Now we can move to regression.%%HIGHLIGHT%% ==The good thing about regression problems is that we can use all the cross-validation techniques mentioned above for regression problems except for stratified k-fold. That is we cannot use stratified k-fold directly, but there are ways to change the problem a bit so that we can use stratified k-fold for regression problems. Mostly, simple k-fold cross-validation works for any regression problem. However, if you see that the distribution of targets is not consistent, you can use stratified k-fold.== %%POSTFIX%%Approaching (Almost) Any Machi*
>%%LINK%%[[#^g6833ssxwa7|show annotation]]
>%%COMMENT%%
>in regression, we can use all the cross-validation technique except for stratified k-fold (we cannot use it directly, mostly we use simple k-fold cross validation for regression, but if the distribution of targets is not consistent, we can use stratified k-fold).
>%%TAGS%%
>
^g6833ssxwa7


>%%
>```annotation-json
>{"created":"2023-02-26T04:20:00.377Z","text":"divide the target into bins to use stratified k-fold in regression.","updated":"2023-02-26T04:20:00.377Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":37452,"end":37634},{"type":"TextQuoteSelector","exact":"To use stratified k-fold for a regression problem, we have first to divide the target into bins, and then we can use stratified k-fold in the same way as for classification problems.","prefix":"g Problem – Abhishek Thakur  27 ","suffix":" There are several choices for s"}]}]}
>```
>%%
>*%%PREFIX%%g Problem – Abhishek Thakur  27%%HIGHLIGHT%% ==To use stratified k-fold for a regression problem, we have first to divide the target into bins, and then we can use stratified k-fold in the same way as for classification problems.== %%POSTFIX%%There are several choices for s*
>%%LINK%%[[#^fr2zpma64ho|show annotation]]
>%%COMMENT%%
>divide the target into bins to use stratified k-fold in regression.
>%%TAGS%%
>
^fr2zpma64ho


>%%
>```annotation-json
>{"created":"2023-02-26T04:20:50.237Z","text":"if you have a lot of samples (>10k or >100k) you don't need to care about the number of bins, just divide the data into 10 or 20 bins. If you do not have a lot of samples, use **Sturge's Rule **to calculate the number of bins.","updated":"2023-02-26T04:20:50.237Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":37706,"end":37974},{"type":"TextQuoteSelector","exact":" If you have a lot of samples( > 10k, > 100k), then you don’t need to care about the number of bins. Just divide the data into 10 or 20 bins. If you do not have a lot of samples, you can use a simple rule like Sturge’s Rule to calculate the appropriate number of bins.","prefix":" the appropriate number of bins.","suffix":"  Sturge’s rule: Number of Bins "}]}]}
>```
>%%
>*%%PREFIX%%the appropriate number of bins.%%HIGHLIGHT%% ==If you have a lot of samples( > 10k, > 100k), then you don’t need to care about the number of bins. Just divide the data into 10 or 20 bins. If you do not have a lot of samples, you can use a simple rule like Sturge’s Rule to calculate the appropriate number of bins.== %%POSTFIX%%Sturge’s rule: Number of Bins*
>%%LINK%%[[#^45ugeocforz|show annotation]]
>%%COMMENT%%
>if you have a lot of samples (>10k or >100k) you don't need to care about the number of bins, just divide the data into 10 or 20 bins. If you do not have a lot of samples, use **Sturge's Rule **to calculate the number of bins.
>%%TAGS%%
>
^45ugeocforz


>%%
>```annotation-json
>{"created":"2023-02-26T04:29:18.249Z","text":"cross-validation is the first and most essential step in ML Model.","updated":"2023-02-26T04:29:18.249Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":40106,"end":40210},{"type":"TextQuoteSelector","exact":"Cross-validation is the first and most essential step when it comes to building machine learning models.","prefix":"g Problem – Abhishek Thakur  29 ","suffix":" If you want to do feature engin"}]}]}
>```
>%%
>*%%PREFIX%%g Problem – Abhishek Thakur  29%%HIGHLIGHT%% ==Cross-validation is the first and most essential step when it comes to building machine learning models.== %%POSTFIX%%If you want to do feature engin*
>%%LINK%%[[#^6hv74hjygly|show annotation]]
>%%COMMENT%%
>cross-validation is the first and most essential step in ML Model.
>%%TAGS%%
>
^6hv74hjygly


>%%
>```annotation-json
>{"created":"2023-02-26T04:34:24.195Z","text":"Although these types of cross-validation techniques can be applied to almost any machine learning problem, but you must still adopt with your data.","updated":"2023-02-26T04:34:24.195Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":40648,"end":40824},{"type":"TextQuoteSelector","exact":"Still, you must keep in mind that cross-validation also depends a lot on the data and you might need to adopt new forms of cross-validation depending on your problem and data. ","prefix":"t any machine learning problem. ","suffix":"  For example, let’s say we have"}]}]}
>```
>%%
>*%%PREFIX%%t any machine learning problem.%%HIGHLIGHT%% ==Still, you must keep in mind that cross-validation also depends a lot on the data and you might need to adopt new forms of cross-validation depending on your problem and data.== %%POSTFIX%%For example, let’s say we have*
>%%LINK%%[[#^eob38usddtk|show annotation]]
>%%COMMENT%%
>Although these types of cross-validation techniques can be applied to almost any machine learning problem, but you must still adopt with your data.
>%%TAGS%%
>
^eob38usddtk


>%%
>```annotation-json
>{"created":"2023-02-26T04:38:01.072Z","text":"another type of cross-validation","updated":"2023-02-26T04:38:01.072Z","document":{"title":"Approaching (Almost) Any Machine Learning Problem","link":[{"href":"urn:x-pdf:6dc67c4755df34a6ce28a3cba658338f"},{"href":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf"}],"documentFingerprint":"6dc67c4755df34a6ce28a3cba658338f"},"uri":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","target":[{"source":"https://raw.githubusercontent.com/abhishekkrthakur/approachingalmost/master/AAAMLP.pdf","selector":[{"type":"TextPositionSelector","start":41445,"end":41455},{"type":"TextQuoteSelector","exact":"GroupKFold","prefix":" of cross-validation known  as  ","suffix":".  Here  the  patients  can  be "}]}]}
>```
>%%
>*%%PREFIX%%of cross-validation known  as%%HIGHLIGHT%% ==GroupKFold== %%POSTFIX%%.  Here  the  patients  can  be*
>%%LINK%%[[#^99h8o3zi96|show annotation]]
>%%COMMENT%%
>another type of cross-validation
>%%TAGS%%
>
^99h8o3zi96
